;
; Copyright (c) 2021 RISC OS Open Limited
; All rights reserved.
;
; Redistribution and use in source and binary forms, with or without
; modification, are permitted provided that the following conditions
; are met:
; 1. Redistributions of source code must retain the above copyright
;    notice, this list of conditions and the following disclaimer.
; 2. Redistributions in binary form must reproduce the above copyright
;    notice, this list of conditions and the following disclaimer in the
;    documentation and/or other materials provided with the distribution.
;
; THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
; ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
; IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
; ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
; FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
; DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
; OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
; HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
; LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
; OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
; SUCH DAMAGE.
;

        GET     Hdr:ListOpts
        GET     Hdr:Macros
        GET     Hdr:System
        GET     Hdr:FSNumbers
        GET     Hdr:NewErrors
        GET     Hdr:VFPSupport

;
; Constraints in this translation of the original:
; * PRESERVE8 stack alignment
; * Only use D0-D15 to allow for D16 VFP units
; * Flush to zero is off
; One day we might be able to write this in C...
;

        EXPORT  fp64log10
        EXPORT  fp64log
        EXPORT  fp64pow
        EXPORT  fp64exp
        IMPORT  RaiseException
        IMPORT  fused64_muladd
        IMPORT  LDExp

        AREA    |power$$Code|, CODE, READONLY, PIC
        ARM

        GET     Macros.s

        ; double fp64log10(double x)
        ; Find k and f, such that x = 2^k * (1 + f)
        ; where 1 + f is in the range [2^-0.5 2^+0.5]
        ; Approximate log(1 + f) using a polynomial, change base to 10.
        ;
        ; Exceptions per ISO9899:2018 F10.3.8
        ;       x = ±0 is a divide by zero
        ;       x < 0 is an invalid operation
        ; Based on a translation to AArch32 of
        ;       openlibm/src/e_log10.c
        ;       Copyright (C) 1993 by Sun Microsystems, Inc. All rights reserved.
        ;       Developed at SunSoft, a Sun Microsystems, Inc. business.
        ;       Permission to use, copy, modify, and distribute this software is
        ;       freely granted, provided that this notice is preserved.
fp64log10
        VMOV    a1, a2, d0
        Push    "v1, lr"
        LDR     v1, =&7FF:SHL:20
        TEQ     a2, v1
        TEQEQ   a1, #0                  ; log(+INF) = +INF
        BEQ     %FT10
        ORRS    a3, a1, a2, LSL #1
        ORREQ   v1, v1, #1:SHL:31       ; log(±0) = -INF
        MOVEQ   a1, #FPSCR_DZC
        BEQ     %FT15
        BIC     a3, a2, #1:SHL:31
        CMP     a3, v1
        CMPEQ   a1, #0
        Pull    "v1, pc",HI             ; Propagate NaNs
        TST     a2, #1:SHL:31
        ORRNE   v1, v1, #1:SHL:19       ; x < 0 => QNaN
        BEQ     %FT20
10
        MOV     a1, #FPSCR_IOC
15
        BL      RaiseException
        MOV     a1, #0
        VMOV    d0, a1, v1
        Pull    "v1, pc"
20
        ; x is not 0, NaN, nor INF, and is positive
        MOV     v1, #0                  ; k
        VLDR    d1, done
        LoadExp64 ip, -1022             ; 2^-1022
        CMP     a2, ip
        CMPEQ   a1, #0
        VLDRCC  d2, dtwo54
        VMULCC.F64 d0, d0, d2
        VMOVCC  a1, a2, d0
        SUBCC   v1, v1, #54             ; Scale up by 2^54 to normalise
        ExpBits64 a3, a2
        LDR     a4, =DBL_EXP_BIAS
        SUB     a3, a3, a4
        ADD     v1, v1, a3              ; k += unbiased exponent
        SUB     ip, ip, #1              ; ip := &FFFFF
        AND     a2, a2, ip
        LDR     a4, =&95F64
        VMOV    a3, s3                  ; High word of done
        ADD     a4, a4, a2
        ANDS    a4, a4, #1:SHL:20
        EOR     a4, a4, a3
        ORR     ip, a2, a4
        VMOV    s1, ip                  ; High word of x
        ADDNE   v1, v1, #1              ; Normalised to x or x/2
        VMOV    s4, v1
        VCVT.F64.S32 d4, s4             ; d4 = (double)k
        VSUB.F64 d0, d0, d1             ; f
30
        ; Remez polynomial
        VPUSH   { d8-d10 }
        ADR     a3, dClogO
        VLDMIA  a3!, { d7-d10 }
        VADD.F64 d2, d1, d1             ; 2
        VADD.F64 d3, d2, d0
        VDIV.F64 d3, d0, d3             ; s = f / (2 + f)
        VLDR    d2, dhalf
        VMUL.F64 d5, d3, d3             ; s^2
        VMUL.F64 d6, d5, d5             ; s^4
        VFused64 d9, d6, d10
        VMLA.F64 d8, d6, d9
        VMLA.F64 d7, d6, d8
        VMUL.F64 d7, d5, d7
        ASSERT  dClogO + (4*8) = dClogE
        VLDMIA  a3, { d8-d10 }
        VFused64 d9, d6, d10
        VMLA.F64 d8, d6, d9
        VMUL.F64 d8, d6, d8
        VADD.F64 d1, d7, d8             ; R = poly
        VPOP    { d8-d10 }

        VLDR    d5, dlog2
        VLDR    d6, dlog2r
        VMUL.F64 d2, d0, d2
        VMUL.F64 d2, d0, d2             ; 1/2 * f^2
        VADD.F64 d7, d2, d1
        VMUL.F64 d7, d3, d7
        VSUB.F64 d1, d0, d2             ; d1 := hi
        MOV     a3, #0
        VMOV    s2, a3                  ; Low word of hi
        VSUB.F64 d0, d0, d1
        VSUB.F64 d0, d0, d2
        VLDR    d2, d1uponln10
        VLDR    d3, d1uponln10r
        VADD.F64 d0, d0, d7             ; d0 := lo

        ; Change of base for (hi,lo) from e to 10
        VMUL.F64 d5, d4, d5
        VADD.F64 d7, d0, d1
        VMUL.F64 d7, d7, d3
        VMLA.F64 d7, d0, d2
        VMLA.F64 d7, d4, d6
        VMLA.F64 d5, d1, d2
        VADD.F64 d0, d5, d7
        Pull    "v1, pc"

dhalf   DCD     &00000000, &3FE00000    ; 1/2
dtwo54  DCD     &00000000, &43500000    ; (double)1<<54
dln2_20 DCD     &FEE00000, &3FE62E42    ; 6.93147180369123816490e-01
dln2_20r DCD    &35793C76, &3DEA39EF    ; Next 17sf of ln(2)
dlog2   DCD     &509F6000, &3FD34413    ; 3.01029995663611771306e-01
dlog2r  DCD     &11F12B36, &3D59FEF3    ; Next 17sf of log(2)
d1uponln10 DCD  &15200000, &3FDBCB7B    ; 4.34294481878168880939e-01
d1uponln10r DCD &CA9AADD5, &3DBB9438    ; Next 17sf of 1/ln(10)

        ; double fp64log(double x)
        ; Find k and f, such that x = 2^k * (1 + f)
        ; where 1 + f is in the range [2^-0.5 2^+0.5]
        ; Approximate log(1 + f) using a polynomial.
        ;
        ; Exceptions per ISO9899:2018 F10.3.7
        ;       x = ±0 is a divide by zero
        ;       x < 0 is an invalid operation
        ; Based on a translation to AArch32 of
        ;       openlibm/src/e_log.c
        ;       Copyright (C) 1993 by Sun Microsystems, Inc. All rights reserved.
        ;       Developed at SunSoft, a Sun Microsystems, Inc. business.
        ;       Permission to use, copy, modify, and distribute this software is
        ;       freely granted, provided that this notice is preserved.
fp64log
        VMOV    a1, a2, d0
        Push    "v1, lr"
        LDR     v1, =&7FF:SHL:20
        TEQ     a2, v1
        TEQEQ   a1, #0                  ; log(+INF) = +INF
        BEQ     %FT10
        ORRS    a3, a1, a2, LSL #1
        ORREQ   v1, v1, #1:SHL:31       ; log(±0) = -INF
        MOVEQ   a1, #FPSCR_DZC
        BEQ     %FT15
        BIC     a3, a2, #1:SHL:31
        CMP     a3, v1
        CMPEQ   a1, #0
        Pull    "v1, pc",HI             ; Propagate NaNs
        TST     a2, #1:SHL:31
        ORRNE   v1, v1, #1:SHL:19       ; x < 0 => QNaN
        BEQ     %FT20
10
        MOV     a1, #FPSCR_IOC
15
        BL      RaiseException
        MOV     a1, #0
        VMOV    d0, a1, v1
        Pull    "v1, pc"
20
        ; x is not 0, NaN, nor INF, and is positive
        MOV     v1, #0                  ; k
        VLDR    d1, done
        LoadExp64 ip, -1022             ; 2^-1022
        CMP     a2, ip
        CMPEQ   a1, #0
        VLDRCC  d2, dtwo54
        VMULCC.F64 d0, d0, d2
        VMOVCC  a1, a2, d0
        SUBCC   v1, v1, #54             ; Scale up by 2^54 to normalise
        ExpBits64 a3, a2
        LDR     a4, =DBL_EXP_BIAS
        SUB     a3, a3, a4
        ADD     v1, v1, a3              ; k += unbiased exponent
        SUB     ip, ip, #1              ; ip := &FFFFF
        AND     a2, a2, ip
        LDR     a4, =&95F64
        VMOV    a3, s3                  ; High word of done
        ADD     a4, a4, a2
        ANDS    a4, a4, #1:SHL:20
        EOR     a4, a4, a3
        ORR     ip, a2, a4
        VMOV    s1, ip                  ; High word of x
        ADDNE   v1, v1, #1              ; Normalised to x or x/2
        VMOV    s4, v1
        VCVT.F64.S32 d4, s4             ; d4 = (double)k
        VSUB.F64 d0, d0, d1             ; f
30
        ; Remez polynomial
        VPUSH   { d8-d10 }
        ADR     a3, dClogO
        VLDMIA  a3!, { d7-d10 }
        VADD.F64 d2, d1, d1             ; 2
        VADD.F64 d3, d2, d0
        VDIV.F64 d3, d0, d3             ; s = f / (2 + f)
        VLDR    d2, dhalf
        VMUL.F64 d5, d3, d3             ; s^2
        VMUL.F64 d6, d5, d5             ; s^4
        VFused64 d9, d6, d10
        VMLA.F64 d8, d6, d9
        VMLA.F64 d7, d6, d8
        VMUL.F64 d7, d5, d7
        ASSERT  dClogO + (4*8) = dClogE
        VLDMIA  a3, { d8-d10 }
        VFused64 d9, d6, d10
        VMLA.F64 d8, d6, d9
        VMUL.F64 d8, d6, d8
        VADD.F64 d1, d7, d8             ; R = poly
        VPOP    { d8-d10 }

        LDR     a3, =&6147A
        LDR     a4, =&6B851
        SUB     a3, a2, a3
        SUB     a4, a4, a2
        ORRS    a2, a3, a4
        VLDR    d5, dln2_20
        VLDR    d6, dln2_20r
        BLE     %FT40

        VMUL.F64 d2, d0, d2
        VMUL.F64 d2, d0, d2             ; 1/2 * f^2
        VADD.F64 d7, d2, d1
        VMUL.F64 d7, d3, d7
        VMLA.F64 d7, d4, d6
        VSUB.F64 d1, d2, d7
        B       %FT50
40
        VSUB.F64 d2, d0, d1
        VMUL.F64 d1, d2, d3             ; s * (f - R)
        VMLS.F64 d1, d4, d6
50
        VMUL.F64 d5, d5, d4
        VSUB.F64 d1, d1, d0
        VSUB.F64 d0, d5, d1             ; log(x) = k * log(2) - (equation - f)
        Pull    "v1, pc"

done    DCD     &00000000, &3FF00000    ; (double)1
dexpmax DCD     &FEFA39EF, &40862E42    ;  7.09782712893383973096e+02
dexpmin DCD     &D52D3051, &c0874910    ; -7.45133219101941108420e+02
de1     DCD     &8B145769, &4005BF0A    ; e^1
dCexp   DCD     &5555553E, &3FC55555    ;  1.66666666666666019037e-01
        DCD     &16BEBD93, &BF66C16C    ; -2.77777777770155933842e-03
        DCD     &AF25DE2C, &3F11566A    ;  6.61375632143793436117e-05
        DCD     &C5D26BF1, &BEBBBD41    ; -1.65339022054652515390e-06
        DCD     &72BEA4D0, &3E663769    ;  4.13813679705723846039e-08
d2pow1023 DCD   &00000000, &7FE00000    ; (double)2^1023
d1uponln2 DCD   &652B82FE, &3FF71547    ;  1.44269504088896338700e+00

dClogO  DCD     &55555593, &3FE55555    ; 6.666666666666735130e-01
        DCD     &94229359, &3FD24924    ; 2.857142874366239149e-01
        DCD     &96CB03DE, &3FC74664    ; 1.818357216161805012e-01
        DCD     &DF3E5244, &3FC2F112    ; 1.479819860511658591e-01
dClogE  DCD     &9997FA04, &3FD99999    ; 3.999999999940941908e-01
        DCD     &1D8E78AF, &3FCC71C5    ; 2.222219843214978396e-01
        DCD     &D078C69F, &3FC39A09    ; 1.531383769920937332e-01

        ; double fp64exp(double y)
        ; Special case e^1 for speed and accuracy.
        ; Find r and k such that    x = k*ln(2) + r
        ;                        |r| <= ln(2)/2
        ; Approximate exp(r) over the range [0 ln(2)/2] then scale back
        ; to          exp(y) = 2^k * exp(r)
        ;
        ; Exceptions per ISO9899:2018 F10.3.1
        ;       None
        ; Based on a translation to AArch32 of
        ;       openlibm/src/e_exp.c
        ;       Copyright (C) 2004 by Sun Microsystems, Inc. All rights reserved.
        ;       Permission to use, copy, modify, and distribute this software is
        ;       freely granted, provided that this notice is preserved.
fp64exp
        LDR     ip, =&7FF:SHL:20
        VMOV    a1, a2, d0
        BIC     a3, a2, #1:SHL:31
        CMP     a3, ip
        CMPEQ   a1, #0
        BXHI    lr                      ; Propagate NaNs
        BNE     %FT10

        TST     a2, #1:SHL:31
        MOVNE   a3, #0
        VMOVNE  d0, a3, a3              ; e^-INF = 0
        BX      lr                      ; e^+INF = +INF
10
        VLDR    d1, dexpmax
        VLDR    d2, dexpmin
        VCMP.F64 d0, d1
        VMRS    APSR_nzcv, FPSCR
        MOVGT   a3, #0
        VMOVGT  d0, a3, ip
        MOVGT   a1, #FPSCR_OFC
        BGT     RaiseException          ; Will overflow

        VCMP.F64 d0, d2
        VMRS    APSR_nzcv, FPSCR
        VSUBLT.F64 d0, d0, d0
        MOVLT   a1, #FPSCR_UFC
        BLT     RaiseException          ; Will underflow

        LoadExp64 ip, 0                 ; 2^0
        TEQ     a2, ip
        TEQEQ   a1, #0
        VLDREQ  d0, de1
        BXEQ    lr

        Push    "v1, lr"
        MOV     v1, #0                  ; k = 0
        VMOV    d2, v1, v1              ; hi = 0.0
        VMOV.F64 d1, d2                 ; lo = 0.0
        LDR     ip, =&3FD62E42          ; Approx 1/2 ln(2)
        CMP     a3, ip
        BLS     %FT30
        VLDR    d5, dln2_20
        VLDR    d6, dln2_20r
        LDR     ip, =&3FF0A2B2          ; Approx 3/2 ln(2)
        CMP     a3, ip
        BCS     %FT20
        ; 1/2 ln(2) < |y| < 3/2 ln(2)
        TST     a2, #1:SHL:31
        VNEGNE.F64 d5, d5
        VNEGNE.F64 d6, d6
        VSUB.F64 d2, d0, d5             ; hi
        VMOV.F64 d1, d6                 ; lo
        ADDEQ   v1, v1, #1
        SUBNE   v1, v1, #1
        B       %FT40
20
        ; |y| >= 3/2 ln(2)
        TST     a2, #1:SHL:31
        VLDR    d3, dhalf
        VLDR    d4, d1uponln2
        VNEGNE.F64 d3, d3
        VMLA.F64 d3, d4, d0
        VCVT.S32.F64 s6, d3             ; k = integer round towards zero
        VMOV    v1, s6
        VCVT.F64.S32 d3, s6
        VMUL.F64 d5, d3, d5
        VSUB.F64 d2, d0, d5             ; hi
        VMUL.F64 d1, d3, d6             ; lo
        B       %FT40
30
        ; |y| <= 1/2 ln(2)
        LoadExp64 ip, -28               ; 2^-28
        CMP     a3, ip
        BCS     %FT50
        ORRS    a3, a1, a2, LSL #1      ; e^0 is exact
        MOVNE   a1, #FPSCR_IXC
        BLNE    RaiseException
        VLDR    d1, done
        VADD.F64 d0, d1, d0
        Pull    "v1, pc"                ; Tiny, so approximate exp(y) = 1 + y
40
        VSUB.F64 d0, d2, d1             ; y = hi - lo
50
        ; 0 <= |y| <= 1/2 ln(2)
        VPUSH   { d8 }
        VMUL.F64 d3, d0, d0             ; y^2
        ADR     a3, dCexp
        VLDMIA  a3, { d4-d8 }
        VFused64 d7, d3, d8
        VFused64 d6, d3, d7
        VMLA.F64 d5, d3, d6
        VMLA.F64 d4, d3, d5
        VPOP    { d8 }
        VLDR    d5, done
        VADD.F64 d6, d5, d5             ; 2
        VMUL.F64 d4, d4, d3
        VSUB.F64 d3, d0, d4             ; d3 := poly
        CMP     v1, #0
        BNE     %FT60

        VSUB.F64 d4, d3, d6             ;                          (poly - 2)
        VMUL.F64 d7, d0, d3             ;               (y * poly)
        VDIV.F64 d7, d7, d4
        VSUB.F64 d7, d7, d0
        VSUB.F64 d0, d5, d7             ; exp(y) = 1 - ((y * poly)/(poly - 2) - x)
        Pull    "v1, pc"
60
        VSUB.F64 d4, d6, d3             ;                                (2 - poly)
        VMUL.F64 d7, d0, d3             ;                     (y * poly)
        VDIV.F64 d7, d7, d4
        VSUB.F64 d1, d1, d7             ;                lo - (y * poly)/(2 - poly)
        VSUB.F64 d1, d1, d2             ;              ((lo - (y * poly)/(2 - poly)) - hi)
        VSUB.F64 d1, d5, d1             ; exp(r) = 1 - ((lo - (y * poly)/(2 - poly)) - hi)

        ; k non zero, need 2^k scaler
        CMP     v1, #1024               ; Careful making an exponent look like a NaN
        VLDREQ  d4, d2pow1023
        VMULEQ.F64 d1, d1, d6
        VMULEQ.F64 d0, d1, d4
        Pull    "v1, pc",EQ

        LDR     ip, =-1021
        LoadExp64 a4, 0                 ; 2^0
        CMP     v1, ip
        ADD     a3, a4, v1, LSL #20     ; Exponent of 2^k
        ADDLT   a3, a3, #1000:SHL:20    ;             2^(k+1000)
        MOV     a2, #0
        VMOV    d2, a2, a3
        VMUL.F64 d0, d1, d2
        LoadExp64 a3, -1000             ; 2^-1000
        VMOVLT  d2, a2, a3
        VMULLT.F64 d0, d0, d2
        Pull    "v1, pc"

        ; double fp64pow(double x, double y)
        ; Take logarithms pow(x, y) = x^y
        ;                 log2(x^y) = y * log2(x)
        ;                    => x^y = 2^(y * log2(x))
        ; Only if y is an integer can x be negative.
        ; Pay extra attention to inaccuracies in log2(x) because they grow quickly.
        ;
        ; Exceptions per ISO9899:2018 F10.4.4
        ;       x = ±0, y < 0 and an odd integer, is a divide by zero
        ;       x = ±0, finite y < 0 and not an odd integer, is a divide by zero
        ;       x = ±0, y = -INF may raise a divide by zero
        ;       finite x < 0, finite y which is not an integer, is an invalid operation
        ; Based on a translation to AArch32 of
        ;       openlibm/src/e_pow.c
        ;       Copyright (C) 2004 by Sun Microsystems, Inc. All rights reserved.
        ;       Permission to use, copy, modify, and distribute this software is
        ;       freely granted, provided that this notice is preserved.
fp64pow
        VMOV    a1, a2, d0
        VMOV    a3, a4, d1
        ORRS    ip, a3, a4, LSL #1
        VLDREQ  d0, done                ; anything^0 = 1
        BXEQ    lr
        LoadExp64 ip, 0                 ; 2^0
        TEQ     a2, ip
        TEQEQ   a1, #0
        BXEQ    lr                      ; 1^anything = 1
        TEQ     a4, ip
        TEQEQ   a3, #0
        BXEQ    lr                      ; anything^1 = itself

yisint  RN v1
absa2   RN v2
absa4   RN v3
scalen  RN v4

        Push    "v1-v5, lr"
        BIC     absa2, a2, #1:SHL:31
        BIC     absa4, a4, #1:SHL:31
        LDR     ip, =&7FF:SHL:20
        CMP     absa2, ip
        CMPEQ   a1, #0
        BHI     %FT10
        CMP     absa4, ip
        CMPEQ   a3, #0
10
        VADDHI.F64 d0, d0, d1           ; Propagate NaNs
        BXHI    lr

        ; x is not 1, y is not 0 or 1, neither are NaN
        MOV     yisint, #0
        TST     a2, #1:SHL:31
        BEQ     %FT25
        LoadExp64 ip, DBL_MANT_DIG
        CMP     absa4, ip
        MOVCS   yisint, #2              ; |y| >= bits in the mantissa
        BCS     %FT25
        LoadExp64 ip, 0                 ; 2^0
        CMP     absa4, ip
        BCC     %FT25                   ; |y| < 1
        MOV     lr, absa4, LSR #20
        SUB     lr, lr, ip, LSR #20     ; Unbiased exponent
        ADD     lr, lr, #1+11           ; Sign and exponent bits
        RSBS    ip, lr, #32
        BLS     %FT15
        ; Shift < 32
        MOVS    v5, a4, LSL lr          ; C = lost bit
        ORR     v5, v5, a3, LSR ip
        MOV     v4, a3, LSL lr
        B       %FT20
15
        ; Shift >= 32
        SUB     ip, lr, #32
        MOVS    v5, a3, LSL ip          ; C = lost bit
        MOV     v4, #0
20
        ORRS    v4, v4, v5              ; Z = 0 if there are fractional bits
        BNE     %FT25
        MOVCC   yisint, #2
        MOVCS   yisint, #1
25
        ; yisint = 0 => y is not an integer
        ; yisint = 1 => y is an odd int
        ; yisint = 2 => y is an even int
        LDR     ip, =&7FF:SHL:20

        ; Pick off interesting values of y
        TEQ     a3, #0
        BNE     %FT35
        LoadExp64 v4, 0                 ; 2^0
        TEQ     absa4, ip
        BNE     %FT30
        ; y = ±INF
        CMP     absa2, v4
        CMPEQ   a1, #0
        ORREQ   ip, ip, #1:SHL:19
        VMOVEQ  d0, a3, ip              ; ±1^±INF => QNaN
        Pull    "v1-v5, pc",EQ
        EORCS   a4, a4, #1:SHL:31
        TST     a4, #1:SHL:31
        VMOVEQ  d0, a3, a3              ; (|x| > 1)^-INF = 0
                                        ; (|x| < 1)^+INF = 0
        VMOVNE  d0, a3, ip              ; (|x| > 1)^+INF = +INF
                                        ; (|x| < 1)^-INF = +INF
        Pull    "v1-v5, pc"
30
        LoadExp64 v4, 1                 ; 2^1
        CMP     a4, v4
        VMULEQ.F64 d0, d0, d0           ; Shortcut x^2
        Pull    "v1-v5, pc",EQ
        LoadExp64 v4, -1                ; 2^-1
        CMP     a4, v4
        TSTEQ   a2, #1:SHL:31
        VSQRTEQ.F64 d0, d0              ; Shortcut x^(1/2) for x positive
        Pull    "v1-v5, pc",EQ
35
        ; Pick off interesting values of x
        TEQ     a1, #0
        BNE     %FT50
        TEQ     absa2, ip
        BNE     %FT40
        ; x = ±INF
        ORRS    v4, a2, a4
        VMOVPL  d0, a1, ip              ; +INF^+anything = +INF
        VMOVMI  d0, a1, a1              ; +INF^-anything = +0
        TST     a2, #1:SHL:31
        VNEGNE.F64 d0, d0               ; -INF^anything = -0
        Pull    "v1-v5, pc"
40
        ORRS    v4, a1, a2, LSL #1
        BNE     %FT50
        ; x = ±0
        BCS     %FT45
        ; x = +0
        TST     a4, #1:SHL:31
        VMOVEQ  d0, a1, a1              ; +0^+anything = +0
        VMOVNE  d0, a1, ip              ; +0^-anything = +INF
        MOVNE   a1, #FPSCR_DZC
        BLNE    RaiseException
        Pull    "v1-v5, pc"
45
        ; x = -0
        TST     a4, #1:SHL:31
        VABSEQ.F64 d0, d0               ; -0^+anything = +0   } anything other than
        VMOVNE  d0, a1, ip              ; -0^-anything = +INF } an odd integer
        MOVNE   a1, #FPSCR_DZC
        BLNE    RaiseException
        CMP     yisint, #1
        VNEGEQ.F64 d0, d0               ; -0^oddint = -0 or -INF
        Pull    "v1-v5, pc"
50
        ; x and y are not 0, 1, ±INF or NaN
        ; A hard value of x^y
        VABS.F64 d0, d0
        ADDS    lr, yisint, a2, ASR #31
        BPL     %FT55
        ORR     ip, ip, #1:SHL:19
        VMOV    d0, yisint, ip          ; -ve^nonint = QNaN
        MOV     a1, #FPSCR_IOC
        Pull    "v1-v5, lr"
        B       RaiseException
55
        LoadExp64 v4, 0                 ; 2^0
        LoadExp64 ip, 64                ; 2^64
        CMP     absa4, ip
        BHI     %FT60                   ; |y| > 2^64 will over/underflow
        LoadExp64 ip, 31                ; 2^31
        CMP     absa4, ip
        BLS     %FT70                   ; Safe
        CMP     absa2, v4
        BEQ     %FT65                   ; Safe
60
        CMP     absa2, v4
        EORCC   a4, a4, #1:SHL:31
        TST     a4, #1:SHL:31
        VLDREQ  d0, dhuge               ; (|x| > 1)^+large = huge
                                        ; (|x| < 1)^-large = huge
        VLDRNE  d0, dtiny               ; (|x| > 1)^-large = tiny
                                        ; (|x| < 1)^+large = tiny
        MOVEQ   a1, #FPSCR_OFC
        MOVNE   a1, #FPSCR_UFC
        BL      RaiseException
        VMUL.F64 d0, d0, d0
        B       %FT999

dCtaylor DCD    &00000000, &3FE00000    ; 1/2
        DCD     &55555555, &3FD55555    ; 1/3
        DCD     &00000000, &3FD00000    ; 1/4
d1uponln2s DCD  &60000000, &3FF71547    ; 1.44269502162933349609e+00
d1uponln2sr DCD &F85DDF44, &3E54AE0B    ; Next 17sf of 1/ln(2)
d2pow53 DCD     &00000000, &43400000    ; (double)2^53
dhuge   DCD     &8800759C, &7E37E43C    ; 10^300
dtiny   DCD     &C2F8F359, &01A56E1F    ; 10^-300
d3over2 DCD     &00000000, &3FF80000    ; 1.5
dClogx  DCD     &33333303, &3FE33333    ; 5.99999999999994648725e-01
        DCD     &DB6FABFF, &3FDB6DB6    ; 4.28571428578550184252e-01
        DCD     &518F264D, &3FD55555    ; 3.33333329818377432918e-01
        DCD     &A91D4101, &3FD17460    ; 2.72728123808534006489e-01
        DCD     &93C9DB65, &3FCD864A    ; 2.30660745775561754067e-01
        DCD     &4A454EEF, &3FCA7E28    ; 2.06975017800338417784e-01

65
        ; |y| > 2^32 but |x| is very close to one (bottom 32 bits of mantissa only)
        ; As |1 - x| <= 2^-20 can approximate log(x) by a Taylor series
        ; expansion t - 1/2 t^2 + 1/3 t^3 - 1/4 t^4
        ;           t - (t^2 * (1/2 - t * (1/3 - t * 1/4)))
        ADR     ip, dCtaylor
        VLDMIA  ip, { d5-d7 }
        VLDR    d4, dunity
        VSUB.F64 d4, d0, d4             ; t
        VMLS.F64 d6, d4, d7
        VMLS.F64 d5, d4, d6
        VMUL.F64 d5, d4, d5
        VMUL.F64 d5, d4, d5
        ADRL    ip, d1uponln2
        VLDR    d3, [ip]
        VLDR    d6, d1uponln2s
        VLDR    d7, d1uponln2sr
        MOV     ip, #0
        ; Change of base to 2
        VMUL.F64 d6, d6, d4
        VMUL.F64 d4, d4, d7
        VMLS.F64 d4, d5, d3
        VADD.F64 d2, d6, d4
        VMOV    s4, ip                  ; Low word of d2
        VSUB.F64 d3, d2, d6
        VSUB.F64 d3, d4, d3
        ; Result is doubledouble (t1+t2) in registers d2 and d3
        B       %FT80
70
        ; |y| <= 2^31
        MOV     scalen, #0
        CMP     absa2, #1:SHL:20        ; Subnormal?
        VLDRCC  d7, d2pow53
        SUBCC   scalen, scalen, #53
        VMULCC.F64 d0, d0, d7
        VMOVCC  absa2, s1               ; High word of scaled x
        LDR     ip, =DBL_EXP_BIAS
        ExpBits64 v5, absa2
        SUB     v5, v5, ip
        ADD     scalen, scalen, v5
        BIC     v5, absa2, #&F0000000
        BIC     v5, v5,    #&0FF00000
        ORR     absa2, v5, ip, LSL #20  ; Normalised x

        ; Determine interval
        MOV     a3, #0
        LDR     ip, =&3988E             ; SQR(3/2)
        CMP     v5, ip
        BLS     %FT75
        LDR     ip, =&BB67A             ; SQR(3)
        CMP     v5, ip
        MOVCC   a3, #1
        ADDCS   scalen, scalen, #1
        SUBCS   absa2, absa2, #1:SHL:20
75
        VMOV    s1, absa2               ; High word of scaled x

        ; Compute ss = s_h + s_l = (x-1)/(x+1) or (x-1.5)/(x+1.5)
        TEQ     a3, #0
        VLDR    d6, dunity
        VMOVEQ.F64 d7, d6
        VLDRNE  d7, d3over2
        VSUB.F64 d4, d0, d7
        VADD.F64 d5, d0, d7
        VDIV.F64 d5, d6, d5
        VMUL.F64 d2, d4, d5             ; ss

        MOV     ip, #0
        VMOV.F64 d3, d2                 ; s_h
        VMOV    s6, ip                  ; Low word of d3
        MOV     lr, absa2, LSR #1
        ORR     lr, lr, #&20000000
        ADD     lr, lr, #1:SHL:19
        ADD     lr, lr, a3, LSL #18
        VMOV    d6, ip, lr
        VSUB.F64 d7, d6, d7
        VSUB.F64 d7, d0, d7
        VMLS.F64 d4, d3, d6
        VMLS.F64 d4, d3, d7
        VMUL.F64 d4, d5, d4             ; s_l

        ; Compute log(ax)
        VPUSH   { d8-d11 }
        ADR     ip, dClogx
        VLDMIA  ip, { d6-d11 }
        VMUL.F64 d5, d2, d2             ; ss^2
        VFused64 d10, d5, d11
        VFused64 d9, d5, d10
        VFused64 d8, d5, d9
        VMLA.F64 d7, d5, d8
        VMLA.F64 d6, d5, d7
        VMUL.F64 d6, d6, d5
        VMUL.F64 d6, d6, d5             ; d6 := poly

        VADD.F64 d7, d3, d2
        VMLA.F64 d6, d4, d7
        VMUL.F64 d5, d3, d3             ; s_h^2
        VLDR    d7, d3over2
        VADD.F64 d7, d7, d7             ; 3
        VADD.F64 d8, d7, d5
        VADD.F64 d8, d8, d6
        MOV     ip, #0
        VMOV    s16, ip                 ; Low word of d8
        VSUB.F64 d9, d8, d7
        VSUB.F64 d9, d9, d5
        VSUB.F64 d9, d6, d9
        VMUL.F64 d10, d3, d8
        VMUL.F64 d11, d4, d8
        VMLA.F64 d11, d9, d2

        VADD.F64 d4, d10, d11
        VMOV    s8, ip                  ; Low word of d4
        VSUB.F64 d5, d4, d10
        VSUB.F64 d5, d11, d5
        VLDR    d6, d2o3ln2
        VLDR    d7, d2o3ln2_28
        VLDR    d8, d2o3ln2_28r
        TEQ     a3, #0
        VLDRNE  d9, dCdp
        VLDRNE  d10, dCdpr
        VMOVEQ  d9, a3, a3
        VMOVEQ  d10, a3, a3
        VMUL.F64 d7, d7, d4

        VMUL.F64 d8, d8, d4
        VMLA.F64 d8, d5, d6
        VADD.F64 d8, d8, d10

        VMOV    s12, scalen
        VCVT.F64.S32 d6, s12
        VADD.F64 d2, d7, d8
        VADD.F64 d2, d2, d9
        VADD.F64 d2, d2, d6
        VMOV    s4, ip                  ; Low word of d2
        VSUB.F64 d4, d2, d6
        VSUB.F64 d4, d4, d9
        VSUB.F64 d4, d4, d7
        VSUB.F64 d3, d8, d4
        ; Result is doubledouble (t1+t2) in registers d2 and d3
        VPOP    { d8-d11 }
80
        ; Split up y into y1+y2 and compute (y1+y2)*(t1+t2)
        MOV     ip, #0
        VMOV    d0, ip, a4              ; Just high word of y
        VSUB.F64 d4, d1, d0
        VMUL.F64 d4, d4, d2
        VMLA.F64 d4, d1, d3             ; product lo
        VMUL.F64 d5, d0, d2             ; product hi
        VADD.F64 d0, d4, d5             ; z = prodhi + prodlo
        VMOV    a3, a4, d0
        BIC     absa4, a4, #1:SHL:31

        MOVS    a4, a4
        BMI     %FT90
        LDR     ip, =&40900000
        CMP     a4, ip
        CMPEQ   a3, #0
        BCC     %FT100
        BHI     %FT85                   ; z > 1024

        VLDR    d6, dovftie
        VADD.F64 d6, d6, d4
        VSUB.F64 d7, d0, d4
        VCMP.F64 d6, d7
        VMRS    APSR_nzcv, FPSCR
        BLE     %FT100                  ; z = 1024 and tie breaker says no overflow
85
        MOV     a1, #FPSCR_OFC
        BL      RaiseException
        VLDR    d0, dhuge
        VMUL.F64 d0, d0, d0
        B       %FT999

dunity  DCD     &00000000, &3FF00000    ; (double)1 for VLDR addressing range

90
        LDR     ip, =&4090CC00
        CMP     absa4, ip
        BCC     %FT100
        LDR     ip, =&C090CC00
        CMP     a4, ip
        CMPEQ   a3, #0
        BNE     %FT95                   ; z < -1075

        VSUB.F64 d7, d0, d5
        VCMP.F64 d4, d7
        VMRS    APSR_nzcv, FPSCR
        BGT     %FT100                  ; z = -1075 and tie breaker says no underflow
95
        MOV     a1, #FPSCR_UFC
        BL      RaiseException
        VLDR    d0, dtiny
        VMUL.F64 d0, d0, d0
        B       %FT999
100
        ; Compute 2^(prodhi + prodlo)
        LDR     ip, =DBL_EXP_BIAS
        LoadExp64 lr, -1                ; 2^-1
        ExpBits64 a3, a4
        SUB     a3, a3, ip
        CMP     absa4, lr
        MOVLS   scalen, #0
        BLS     %FT105

        ; |z| > 0.5
        MOV     lr, #1:SHL:20
        ADD     v5, a3, #1
        ADD     scalen, a4, lr, LSR v5  ; New scale
        BIC     a3, scalen, #1:SHL:31
        RSB     a3, ip, a3, LSR #20     ; New exponent for scale
        SUB     lr, lr, #1              ; => &FFFFF
        BIC     v5, scalen, lr, LSR a3
        MOV     ip, #0
        VMOV    d2, ip, v5
        AND     scalen, scalen, lr
        ORR     scalen, scalen, #1:SHL:20
        RSB     a3, a3, #20
        MOV     scalen, scalen, LSR a3
        TST     a4, #1:SHL:31
        RSBNE   scalen, scalen, #0
        VSUB.F64 d5, d5, d2
105
        ; |z| <= 0.5
        VLDR    d1, dln2
        VLDR    d2, dln2_32
        VLDR    d3, dln2_32r
        MOV     ip, #0
        VADD.F64 d0, d4, d5
        VMOV    s0, ip                  ; Low word of d0
        VSUB.F64 d6, d0, d5
        VSUB.F64 d6, d4, d6
        VMUL.F64 d1, d1, d6
        VMLA.F64 d1, d0, d3
        VMUL.F64 d3, d0, d2
        VADD.F64 d2, d3, d1             ; z = u + v
        VSUB.F64 d4, d2, d3
        VSUB.F64 d1, d1, d4

        ; Evaluate polynomial
        ADR     ip, dCpow
        VLDMIA  ip, { d3-d7 }
        VMUL.F64 d0, d2, d2             ; z^2
        VFused64 d6, d0, d7
        VFused64 d5, d0, d6
        VFused64 d4, d0, d5
        VMLA.F64 d3, d0, d4
        VMUL.F64 d3, d0, d3
        VSUB.F64 d3, d2, d3             ; d3 := poly

        VLDR    d7, dunity
        VADD.F64 d6, d7, d7             ; 2
        VMLA.F64 d1, d2, d1
        VMUL.F64 d4, d2, d3
        VSUB.F64 d5, d3, d6
        VDIV.F64 d4, d4, d5
        VSUB.F64 d4, d4, d1             ; r
        VSUB.F64 d2, d4, d2
        VSUB.F64 d2, d7, d2             ; d2 := 1 - (r - z)

        ; Final exponent adjustment
        VMOV    a3, a4, d2
        ADD     a4, a4, scalen, LSL #20
        MOVS    lr, a4, ASR #20
        ASSERT  yisint <> v5
        MOV     v5, a2
        VMOVGT  d0, a3, a4              ; Normal
        MOVLE   a1, scalen
        VMOVLE.F64 d0, d2
        BLLE    LDExp                   ; Scale subnormal
        MOV     a2, v5
999
        ; Fixup sign
        TST     a2, #1:SHL:31
        Pull    "v1-v5, pc",EQ          ; +ve^anything = +ve
        CMP     yisint, #1
        VNEGEQ.F64 d0, d0               ; -ve^oddint  = -ve
        Pull    "v1-v5, pc"

d2o3ln2 DCD     &DC3A03FD, &3FEEC709    ; 2/(3 * ln(2))
d2o3ln2_28 DCD  &E0000000, &3FEEC709    ; Truncated d2o3ln2
d2o3ln2_28r DCD &145B01F5, &BE3E2FE0    ; Next 17sf of truncated d2o3ln2
dovftie DCD     &652B82FE, &3C971547    ; -(1024-log2(ovfl+.5ulp))
dCdp    DCD     &40000000, &3FE2B803    ; 5.84962487220764160156e-01
dCdpr   DCD     &43CFD006, &3E4CFDEB    ; Next 17sf of dCdp
dln2    DCD     &FEFA39EF, &3FE62E42    ;  6.93147180559945286227e-01
dln2_32 DCD     &00000000, &3FE62E43    ;  6.93147182464599609375e-01
dln2_32r DCD    &0CA86C39, &BE205C61    ; -1.90465429995776804525e-09
dCpow   DCD     &5555553E, &3FC55555    ;  1.66666666666666019037e-01
        DCD     &16BEBD93, &BF66C16C    ; -2.77777777770155933842e-03
        DCD     &AF25DE2C, &3F11566A    ;  6.61375632143793436117e-05
        DCD     &C5D26BF1, &BEBBBD41    ; -1.65339022054652515390e-06
        DCD     &72BEA4D0, &3E663769    ;  4.13813679705723846039e-08

        END
